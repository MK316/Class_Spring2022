{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPj8ObDqafZIUiL5tOXTF6s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MK316/Class_Spring2022/blob/main/GetPP_NewAnalysis0815.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸŒ± GetPP-newanalysis (0815 9PM)"
      ],
      "metadata": {
        "id": "G5u3q_Kh6mdQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[1] Read data '04-datatoprocess-0815.csv'\n",
        "\n",
        "- This data has getpp beppresults\n",
        "- We'll add byphrase counts"
      ],
      "metadata": {
        "id": "mVAyioqu7ZUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('04-datatoprocess-0815.csv', encoding='utf-8')\n",
        "data.head()"
      ],
      "metadata": {
        "id": "j6CfHoV767jL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New columns Getpp-by and Bepp-by; NBepp, NGetpp"
      ],
      "metadata": {
        "id": "Omwip8VY-87w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOc2b5li5232"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Load your DataFrame (replace 'yourfile.csv' with your actual file)\n",
        "df = pd.read_csv('04-datatoprocess-0815.csv')\n",
        "\n",
        "# Function to collect sentences that have a \"by\" phrase\n",
        "def extract_by_phrase(sentences):\n",
        "    by_sentences = [sentence for sentence in sentences if re.search(r'\\bby\\b', sentence, re.IGNORECASE)]\n",
        "    return by_sentences\n",
        "\n",
        "# Step 1: Ensure the 'Bepp' and 'Getpp' columns are lists of sentences\n",
        "df['Bepp'] = df['Bepp'].apply(lambda x: eval(x) if isinstance(x, str) else x)\n",
        "df['Getpp'] = df['Getpp'].apply(lambda x: eval(x) if isinstance(x, str) else x)\n",
        "\n",
        "# Step 2: Collect sentences with \"by\" phrase from 'Bepp' and 'Getpp' columns\n",
        "df['Bepp-by'] = df['Bepp'].apply(extract_by_phrase)\n",
        "df['Getpp-by'] = df['Getpp'].apply(extract_by_phrase)\n",
        "\n",
        "# Step 3: Count the number of sentences with \"by\" phrase and store in new columns 'NBepp-by' and 'NGetpp-by'\n",
        "df['NBepp-by'] = df['Bepp-by'].apply(len)\n",
        "df['NGetpp-by'] = df['Getpp-by'].apply(len)\n",
        "\n",
        "# Step 4: Save the final DataFrame to an Excel file\n",
        "df.to_excel('/content/05-newresults.xlsx', index=False)\n",
        "\n",
        "# Optional: Download the Excel file to your local machine in Colab\n",
        "from google.colab import files\n",
        "files.download('/content/05-newresults.xlsx')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "q6a4svCO_dqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Descriptive statistics (0815 9PM)"
      ],
      "metadata": {
        "id": "aXnlIaG9ADqz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[final data](https://raw.githubusercontent.com/MK316/Getpp24/main/data/resultall0815-light.csv) 0815 9:33PM\n",
        "\n",
        "+ The data has NSent, Getpp, Bepp, Counts of by-phrase and its list."
      ],
      "metadata": {
        "id": "HF270unRBvAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/MK316/Getpp24/main/data/resultall0815-light.csv\"\n",
        "\n",
        "# Step 1: Read the dataframe from the provided URL\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Step 2: Sum the values in 'Nbepp' and 'Ngetpp' columns\n",
        "total_nbepp = df['Nbepp'].sum()\n",
        "total_ngetpp = df['Ngetpp'].sum()\n",
        "total_nsent = df['NumSent'].sum()\n",
        "total_active = total_nsent - (total_nbepp + total_ngetpp)\n",
        "total_passive = total_nbepp + total_ngetpp\n",
        "\n",
        "# Display the results\n",
        "print(f\"Total Nbepp: {total_nbepp}\")\n",
        "print(f\"Total Ngetpp: {total_ngetpp}\")\n",
        "print(f\"Total Numsent: {total_nsent}\")\n",
        "print(f\"Total Passives: {total_passive}\")\n",
        "print(f\"Total Actives: {total_active}\")"
      ],
      "metadata": {
        "id": "JWwT5RCvAJD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming 'df' is your DataFrame that has been loaded already\n",
        "\n",
        "# Group by 'register' and sum 'Nbepp' and 'Ngetpp' for each register\n",
        "summary_by_register = df.groupby('register')[['Nbepp', 'Ngetpp']].sum()\n",
        "\n",
        "# Add a row for the total sums of each column\n",
        "summary_by_register.loc['Total'] = summary_by_register.sum()\n",
        "\n",
        "# Add a new column that shows the row-wise sum (Nbepp + Ngetpp) for each register\n",
        "summary_by_register['RowSum'] = summary_by_register.sum(axis=1)\n",
        "\n",
        "# Display the result\n",
        "print(summary_by_register)\n"
      ],
      "metadata": {
        "id": "IDQf4vXPEprB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pie chart"
      ],
      "metadata": {
        "id": "B27FrK8IDUB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "total_nbepp = df['Nbepp'].sum()\n",
        "total_ngetpp = df['Ngetpp'].sum()\n",
        "total_nsent = df['NumSent'].sum()\n",
        "total_active = total_nsent - (total_nbepp + total_ngetpp)\n",
        "total_passive = total_nbepp + total_ngetpp\n",
        "\n",
        "\n",
        "# Data provided\n",
        "total_sentences = total_nsent\n",
        "be_passive = total_nbepp\n",
        "get_passive = total_ngetpp\n",
        "passive_total = total_passive\n",
        "active = total_active\n",
        "\n",
        "# Data for pie charts\n",
        "# Left Pie Chart: Active vs Passive\n",
        "labels_left = ['Active Voice', 'Passive Voice']\n",
        "sizes_left = [active, passive_total]\n",
        "\n",
        "# Right Pie Chart: Be-Passive vs Get-Passive\n",
        "labels_right = ['Be-Passive', 'Get-Passive']\n",
        "sizes_right = [be_passive, get_passive]\n",
        "\n",
        "# Step 2: Create Pie Charts with 'darkblue' and 'orange' colors\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Function to determine text color based on background\n",
        "def get_text_color(background_color):\n",
        "    # Explicitly check if the color is darkblue\n",
        "    if background_color == 'darkblue' or background_color == (0.0, 0.0, 0.5450980392156862, 1.0):\n",
        "        return 'white'\n",
        "    else:\n",
        "        return 'black'\n",
        "\n",
        "# Left Pie Chart (Active vs Passive)\n",
        "plt.subplot(1, 2, 1)\n",
        "wedges_left, texts_left, autotexts_left = plt.pie(\n",
        "    sizes_left, labels=labels_left, autopct='%1.1f%%', colors=['darkblue', 'orange'], textprops={'fontsize': 14}\n",
        ")\n",
        "for i, autotext in enumerate(autotexts_left):\n",
        "    autotext.set_color(get_text_color(wedges_left[i].get_facecolor()))\n",
        "\n",
        "plt.title('(a) Active vs Passive Voice', fontsize=16)\n",
        "\n",
        "# Right Pie Chart (Be-Passive vs Get-Passive)\n",
        "plt.subplot(1, 2, 2)\n",
        "wedges_right, texts_right, autotexts_right = plt.pie(\n",
        "    sizes_right, labels=labels_right, autopct='%1.1f%%', colors=['darkblue', 'orange'], textprops={'fontsize': 14}\n",
        ")\n",
        "for i, autotext in enumerate(autotexts_right):\n",
        "    autotext.set_color(get_text_color(wedges_right[i].get_facecolor()))\n",
        "\n",
        "plt.title('(b) Be-Passive vs Get-Passive Voice', fontsize=16)\n",
        "\n",
        "# Save the current figure in high resolution\n",
        "plt.savefig('/content/pie_charts_high_res.png', dpi=300, bbox_inches='tight')\n",
        "\n",
        "# Adjust the overall layout and make the pie charts smaller\n",
        "plt.tight_layout(pad=3.0)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QDDS30vlDV4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chi-squared test"
      ],
      "metadata": {
        "id": "g0UC-bf-FZ7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Step 1: Create the contingency table based on the provided data\n",
        "# The table should reflect the actual counts of occurrences, not percentages\n",
        "\n",
        "# The contingency table:\n",
        "#            | Be-passive | Get-passive |\n",
        "# --------------------------------------\n",
        "# Written    |    12690   |    132      |\n",
        "# Spoken     |    9547    |    371      |\n",
        "\n",
        "contingency_table = np.array([[12690, 132], [9547, 371]])\n",
        "\n",
        "# Step 2: Perform the Chi-squared test\n",
        "chi2, p, dof, expected = stats.chi2_contingency(contingency_table)\n",
        "\n",
        "# Step 3: Output the results\n",
        "print(f\"Chi-squared Statistic: {chi2}\")\n",
        "print(f\"p-value: {p}\")\n",
        "print(f\"Degrees of Freedom: {dof}\")\n",
        "print(\"Expected Frequencies:\")\n",
        "print(expected)\n"
      ],
      "metadata": {
        "id": "OfD5z7bzFbL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Sample DataFrame (replace this with your actual data)\n",
        "data = {'Register': ['Written', 'Written', 'Spoken', 'Spoken'],\n",
        "        'Type of Passive': ['Be-passive', 'Get-passive', 'Be-passive', 'Get-passive'],\n",
        "        'Observed': [12690, 132, 9547, 371]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Step 1: Create the contingency table (Observed frequencies in a 2x2 matrix)\n",
        "contingency_table = pd.pivot_table(df, values='Observed', index='Register', columns='Type of Passive')\n",
        "\n",
        "# Step 2: Perform chi-square test and get expected frequencies\n",
        "chi2_stat, p_value, dof, expected = chi2_contingency(contingency_table)\n",
        "\n",
        "# Step 3: Convert expected frequencies into a DataFrame with the same structure as contingency_table\n",
        "expected_df = pd.DataFrame(expected, index=contingency_table.index, columns=contingency_table.columns)\n",
        "\n",
        "# Step 4: Calculate residuals and contributions\n",
        "residuals = contingency_table - expected_df  # Raw residuals\n",
        "standardized_residuals = residuals / np.sqrt(expected_df)  # Standardized residuals\n",
        "contributions = (residuals ** 2) / expected_df  # Contribution to chi-square\n",
        "\n",
        "# Step 5: Display the results\n",
        "print(\"Observed Frequencies:\")\n",
        "print(contingency_table)\n",
        "\n",
        "print(\"\\nExpected Frequencies:\")\n",
        "print(expected_df)\n",
        "\n",
        "print(\"\\nRaw Residuals:\")\n",
        "print(residuals)\n",
        "\n",
        "print(\"\\nStandardized Residuals:\")\n",
        "print(standardized_residuals)\n",
        "\n",
        "print(\"\\nContributions to Chi-square:\")\n",
        "print(contributions)\n",
        "\n",
        "# Step 6: Save the results to a CSV file if needed\n",
        "results = pd.DataFrame({\n",
        "    'Observed': contingency_table.stack(),\n",
        "    'Expected': expected_df.stack(),\n",
        "    'Raw Residual': residuals.stack(),\n",
        "    'Standardized Residual': standardized_residuals.stack(),\n",
        "    'Contribution to Chi-square': contributions.stack()\n",
        "})\n",
        "\n",
        "# Save the results to a CSV\n",
        "results.to_csv('/content/chi_square_results_ordered.csv')\n",
        "\n",
        "# Optional: Download the CSV file in Colab\n",
        "from google.colab import files\n",
        "files.download('/content/chi_square_results_ordered.csv')\n"
      ],
      "metadata": {
        "id": "18gp_HvrJiJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Associate plot"
      ],
      "metadata": {
        "id": "IwFdcTf1Ln4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Sample DataFrame (replace this with your actual data)\n",
        "data = {'Register': ['Written', 'Written', 'Spoken', 'Spoken'],\n",
        "        'Type of Passive': ['Be-passive', 'Get-passive', 'Be-passive', 'Get-passive'],\n",
        "        'Observed': [12690, 132, 9547, 371]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Step 1: Create the contingency table (Observed frequencies in a 2x2 matrix)\n",
        "contingency_table = pd.pivot_table(df, values='Observed', index='Register', columns='Type of Passive')\n",
        "\n",
        "# Step 2: Perform chi-square test and get expected frequencies\n",
        "chi2_stat, p_value, dof, expected = chi2_contingency(contingency_table)\n",
        "\n",
        "# Step 3: Convert expected frequencies into a DataFrame with the same structure as contingency_table\n",
        "expected_df = pd.DataFrame(expected, index=contingency_table.index, columns=contingency_table.columns)\n",
        "\n",
        "# Step 4: Calculate residuals, standardized residuals, and contributions\n",
        "residuals = contingency_table - expected_df  # Raw residuals\n",
        "standardized_residuals = residuals / np.sqrt(expected_df)  # Standardized residuals\n",
        "contributions = (residuals ** 2) / expected_df  # Contribution to chi-square\n",
        "\n",
        "# Step 5: Prepare data for plotting standardized residuals\n",
        "std_residuals_flat = standardized_residuals.stack().reset_index(name='Standardized Residuals')\n",
        "contributions_flat = contributions.stack().reset_index(name='Contribution to Chi-square')\n",
        "\n",
        "# Step 6: Plot heatmap of standardized residuals\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create a heatmap for standardized residuals\n",
        "pivot_std_residuals = standardized_residuals.reset_index().melt(id_vars='Register', var_name='Type of Passive', value_name='Standardized Residuals')\n",
        "pivot_contributions = contributions.reset_index().melt(id_vars='Register', var_name='Type of Passive', value_name='Contribution to Chi-square')\n",
        "\n",
        "# Plot heatmap of standardized residuals\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(standardized_residuals, annot=True, cmap='coolwarm', center=0, linewidths=.5, fmt=\".2f\")\n",
        "plt.title(\"Heatmap of Standardized Residuals\", fontsize=16)\n",
        "plt.show()\n",
        "\n",
        "# Step 7: Plot bar plot of contributions to chi-square\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(x='Register', y='Contribution to Chi-square', hue='Type of Passive', data=pivot_contributions, ci=None, palette='Blues', edgecolor='black')\n",
        "\n",
        "# Add labels and title\n",
        "plt.title(\"Contributions to Chi-square by Register and Passive Type\", fontsize=16)\n",
        "plt.xlabel(\"Register\", fontsize=14)\n",
        "plt.ylabel(\"Contribution to Chi-square\", fontsize=14)\n",
        "plt.legend(title='Type of Passive', loc='upper right')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "CXxw32GQLp0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Sample DataFrame (replace this with your actual data)\n",
        "data = {'Register': ['Written', 'Written', 'Spoken', 'Spoken'],\n",
        "        'Type of Passive': ['Be-passive', 'Get-passive', 'Be-passive', 'Get-passive'],\n",
        "        'Observed': [12690, 132, 9547, 371]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Step 1: Create the contingency table (Observed frequencies in a 2x2 matrix)\n",
        "contingency_table = pd.pivot_table(df, values='Observed', index='Register', columns='Type of Passive')\n",
        "\n",
        "# Step 2: Perform chi-square test and get expected frequencies\n",
        "chi2_stat, p_value, dof, expected = chi2_contingency(contingency_table)\n",
        "\n",
        "# Step 3: Convert expected frequencies into a DataFrame with the same structure as contingency_table\n",
        "expected_df = pd.DataFrame(expected, index=contingency_table.index, columns=contingency_table.columns)\n",
        "\n",
        "# Step 4: Calculate standardized residuals\n",
        "residuals = contingency_table - expected_df  # Raw residuals\n",
        "standardized_residuals = residuals / np.sqrt(expected_df)  # Standardized residuals\n",
        "\n",
        "# Step 5: Prepare data for plotting\n",
        "std_residuals_flat = standardized_residuals.stack().reset_index(name='Standardized Residuals')\n",
        "\n",
        "# Step 6: Plot the diverging bar plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create a diverging bar plot\n",
        "sns.barplot(x='Standardized Residuals', y='Register', hue='Type of Passive',\n",
        "            data=std_residuals_flat, palette='rainbow', edgecolor='black')\n",
        "\n",
        "# Add labels and title\n",
        "plt.axvline(0, color='black', linewidth=1)  # Add a vertical line at 0\n",
        "plt.title(\"Diverging Bar Plot of Standardized Residuals\", fontsize=16)\n",
        "plt.xlabel(\"Standardized Residuals\", fontsize=14)\n",
        "plt.ylabel(\"Register\", fontsize=14)\n",
        "plt.legend(title='Type of Passive', loc='upper right')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_QQv3umpMbp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Sample DataFrame (replace this with your actual data)\n",
        "data = {'Register': ['Written', 'Written', 'Spoken', 'Spoken'],\n",
        "        'Type of Passive': ['Be-passive', 'Get-passive', 'Be-passive', 'Get-passive'],\n",
        "        'Observed': [12690, 132, 9547, 371]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Step 1: Create the contingency table (Observed frequencies in a 2x2 matrix)\n",
        "contingency_table = pd.pivot_table(df, values='Observed', index='Register', columns='Type of Passive')\n",
        "\n",
        "# Step 2: Perform chi-square test and get expected frequencies\n",
        "chi2_stat, p_value, dof, expected = chi2_contingency(contingency_table)\n",
        "\n",
        "# Step 3: Convert expected frequencies into a DataFrame with the same structure as contingency_table\n",
        "expected_df = pd.DataFrame(expected, index=contingency_table.index, columns=contingency_table.columns)\n",
        "\n",
        "# Step 4: Calculate standardized residuals\n",
        "residuals = contingency_table - expected_df  # Raw residuals\n",
        "standardized_residuals = residuals / np.sqrt(expected_df)  # Standardized residuals\n",
        "\n",
        "# Step 5: Prepare data for plotting\n",
        "std_residuals_flat = standardized_residuals.stack().reset_index(name='Standardized Residuals')\n",
        "\n",
        "# Step 6: Plot the diverging bar plot with custom bright and dark colors\n",
        "plt.figure(figsize=(12, 5))\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create a custom color palette with bright and dark contrast (e.g., yellow and dark blue)\n",
        "custom_palette = ['#FFD700', '#00008B']  # Bright yellow and dark blue\n",
        "\n",
        "# Create a diverging bar plot with the custom palette\n",
        "sns.barplot(x='Standardized Residuals', y='Register', hue='Type of Passive',\n",
        "            data=std_residuals_flat, palette=custom_palette, edgecolor='black')\n",
        "\n",
        "# Add labels and title\n",
        "plt.axvline(0, color='black', linewidth=1)  # Add a vertical line at 0\n",
        "plt.title(\"Diverging Bar Plot of Standardized Residuals\", fontsize=16)\n",
        "plt.xlim(-10,10)\n",
        "plt.xlabel(\"Standardized Residuals\", fontsize=16)\n",
        "plt.ylabel(\"Register\", fontsize=16)\n",
        "\n",
        "# Customize the legend\n",
        "plt.legend(title='Type of Passive', loc='upper right')\n",
        "\n",
        "# Save the current figure in high resolution\n",
        "plt.savefig('/content/diverging_bar.png', dpi=300, bbox_inches='tight')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vbOhn35hNBmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# By-phrase analysis"
      ],
      "metadata": {
        "id": "7m6LwIQQfC__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/MK316/Getpp24/main/data/resultall0815-light.csv\"\n",
        "\n",
        "# Step 1: Read the dataframe from the provided URL\n",
        "df = pd.read_csv(url)"
      ],
      "metadata": {
        "id": "3X9p5h3JfFqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "oWsUfnA3fXp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming your dataframe is named df and contains 'NBepp-by', 'NGetpp-by', and 'register' columns\n",
        "\n",
        "# Step 1: Create a contingency table with row sums and column sums\n",
        "contingency_table = df.pivot_table(values=['NBepp-by', 'NGetpp-by'], index='register', aggfunc='sum', margins=True, margins_name='Total')\n",
        "\n",
        "# Step 2: Display the contingency table\n",
        "print(contingency_table)\n"
      ],
      "metadata": {
        "id": "-6_QQV0TfuX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming your dataframe is named df and contains 'NBepp', 'NGetpp', 'NBepp-by', 'NGetpp-by', and 'register' columns\n",
        "\n",
        "# Step 1: Group by 'register' and summarize 'NBepp', 'NGetpp', 'NBepp-by', 'NGetpp-by' using the count method\n",
        "count_summary = df.groupby('register')[['Nbepp', 'Ngetpp', 'NBepp-by', 'NGetpp-by']].sum()\n",
        "\n",
        "# Step 2: Display the count summary\n",
        "print(count_summary)\n"
      ],
      "metadata": {
        "id": "wX1gkwRjgvtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bepp = 9547+12690\n",
        "getpp = 371+132\n",
        "BWB = 1464+2725\n",
        "GWB = 37+12\n",
        "BWTB = bepp - BWB\n",
        "GWTB = getpp - GWB\n",
        "\n",
        "print(f\"Be-passive: {bepp}\")\n",
        "print(f\"Get-passive: {getpp}\")\n",
        "print(f\"Be-passive by-phrase: {BWB}\")\n",
        "print(f\"Get-passive by-phrase: {GWB}\")\n",
        "print(f\"Be-passive without-by-phrase: {BWTB}\")\n",
        "print(f\"Get-passive without-by-phrase: {GWTB}\")\n"
      ],
      "metadata": {
        "id": "h-bsu_RvsaEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = 22740\n",
        "4238/t"
      ],
      "metadata": {
        "id": "l2VYB-STxl3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = 503\n",
        "454/t"
      ],
      "metadata": {
        "id": "Gm87-dPJwKTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming your dataframe is named df and contains 'NBepp', 'NGetpp', 'NBepp-by', 'NGetpp-by', and 'register' columns\n",
        "\n",
        "# Step 1: Group by 'register' and sum the relevant columns\n",
        "grouped_data = df.groupby('register')[['Nbepp', 'Ngetpp', 'NBepp-by', 'NGetpp-by']].sum()\n",
        "\n",
        "# Step 2: Calculate the percentage of 'by-phrase' occurrences for Be-passive and Get-passive\n",
        "grouped_data['Be-passive by-phrase (%)'] = (grouped_data['NBepp-by'] / grouped_data['Nbepp']) * 100\n",
        "grouped_data['Get-passive by-phrase (%)'] = (grouped_data['NGetpp-by'] / grouped_data['Ngetpp']) * 100\n",
        "\n",
        "# Step 3: Display the result with the calculated percentages\n",
        "print(grouped_data[['Be-passive by-phrase (%)', 'Get-passive by-phrase (%)']])\n"
      ],
      "metadata": {
        "id": "QUmkr8muhdlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chi-squared test"
      ],
      "metadata": {
        "id": "vSM8d_idinn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Create contingency tables for Be-passive and Get-passive (by-phrase vs no by-phrase)\n",
        "data = {\n",
        "    'NBepp-by': [1464, 2725],  # by-phrase counts for Be-passive in spoken and written\n",
        "    'NBepp-no-by': [9547 - 1464, 12690 - 2725],  # no by-phrase counts for Be-passive\n",
        "    'NGetpp-by': [37, 12],  # by-phrase counts for Get-passive\n",
        "    'NGetpp-no-by': [371 - 37, 132 - 12]  # no by-phrase counts for Get-passive\n",
        "}\n",
        "df = pd.DataFrame(data, index=['Spoken', 'Written'])\n",
        "\n",
        "# Chi-squared test for Be-passive\n",
        "bepp_table = df[['NBepp-by', 'NBepp-no-by']]\n",
        "chi2_bepp_stat, p_bepp_value, dof_bepp, expected_bepp = chi2_contingency(bepp_table)\n",
        "\n",
        "# Chi-squared test for Get-passive\n",
        "getpp_table = df[['NGetpp-by', 'NGetpp-no-by']]\n",
        "chi2_getpp_stat, p_getpp_value, dof_getpp, expected_getpp = chi2_contingency(getpp_table)\n",
        "\n",
        "# Print results\n",
        "print(f\"Be-passive Chi-squared: {chi2_bepp_stat}, p-value: {p_bepp_value}\")\n",
        "print(f\"Get-passive Chi-squared: {chi2_getpp_stat}, p-value: {p_getpp_value}\")\n"
      ],
      "metadata": {
        "id": "l5_Pila2ipQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chi-squared test: Be vs. Get / with vs without"
      ],
      "metadata": {
        "id": "aJPbEnjl0DiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Create the data in a dictionary\n",
        "data = {\n",
        "    \"With by-phrase\": [4189, 49],  # Passive counts\n",
        "    \"Without by-phrase\": [18048, 454]  # Active counts\n",
        "}\n",
        "\n",
        "# Convert the dictionary to a DataFrame\n",
        "df = pd.DataFrame(data, index=[\"Be Passive\", \"Get Passive\"])\n",
        "\n",
        "# Display the contingency table\n",
        "print(\"Contingency Table:\")\n",
        "print(df)\n",
        "\n",
        "# Perform the Chi-square test\n",
        "chi2, p, dof, expected = chi2_contingency(df)\n",
        "\n",
        "# Print the results\n",
        "print(\"\\nChi-square Test Results:\")\n",
        "print(f\"Chi-square Statistic: {chi2:.2f}\")\n",
        "print(f\"Degrees of Freedom: {dof}\")\n",
        "print(f\"P-value: {p:.4f}\")\n",
        "print(\"Expected Frequencies:\")\n",
        "print(pd.DataFrame(expected, index=[\"Be Passive\", \"Get Passive\"], columns=[\"With by-phrase\", \"Without by-phrase\"]))\n"
      ],
      "metadata": {
        "id": "oDLivusv0J8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Create the data in a dictionary\n",
        "data = {\n",
        "    \"Passive\": [4189, 49],  # Passive counts\n",
        "    \"Active\": [18048, 454]  # Active counts\n",
        "}\n",
        "\n",
        "# Convert the dictionary to a DataFrame\n",
        "df = pd.DataFrame(data, index=[\"Be Passive\", \"Get Passive\"])\n",
        "\n",
        "# Display the contingency table\n",
        "print(\"Contingency Table:\")\n",
        "print(df)\n",
        "\n",
        "# Perform the Chi-square test\n",
        "chi2, p, dof, expected = chi2_contingency(df)\n",
        "\n",
        "# Print the results\n",
        "print(\"\\nChi-square Test Results:\")\n",
        "print(f\"Chi-square Statistic: {chi2:.2f}\")\n",
        "print(f\"Degrees of Freedom: {dof}\")\n",
        "print(f\"P-value: {p:.4f}\")\n",
        "print(\"Expected Frequencies:\")\n",
        "print(pd.DataFrame(expected, index=[\"Be Passive\", \"Get Passive\"], columns=[\"Passive\", \"Active\"]))\n"
      ],
      "metadata": {
        "id": "Y76nPNV4zpYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Data: manually input values from your results\n",
        "# 'With By-phrase' and 'Without By-phrase' counts for Be-passive and Get-passive\n",
        "data = {'With By-phrase': [2725, 12],  # NBepp-by for Be-passive, NGetpp-by for Get-passive\n",
        "        'Without By-phrase': [12690 - 2725, 132 - 12]}  # NBepp - NBepp-by for Be-passive, NGetpp - NGetpp-by for Get-passive\n",
        "\n",
        "# Create a DataFrame to represent the contingency table\n",
        "contingency_table = pd.DataFrame(data, index=['Be-passive', 'Get-passive'])\n",
        "\n",
        "# Conduct the chi-squared test\n",
        "chi2_stat, p_value, dof, expected = chi2_contingency(contingency_table)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Chi-squared statistic: {chi2_stat}\")\n",
        "print(f\"p-value: {p_value}\")\n",
        "print(f\"Degrees of freedom: {dof}\")\n",
        "print(\"Expected frequencies:\")\n",
        "print(expected)\n"
      ],
      "metadata": {
        "id": "kWZ8akmIjZH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot"
      ],
      "metadata": {
        "id": "VpbAfhWMlkOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Step 1: Create the observed and expected data (manually input values)\n",
        "observed_data = {\n",
        "    'Register': ['Spoken', 'Spoken', 'Written', 'Written'],\n",
        "    'Passive Type': ['Be-passive', 'Get-passive', 'Be-passive', 'Get-passive'],\n",
        "    'With By-phrase': [1464, 37, 2725, 12],\n",
        "    'Without By-phrase': [9547 - 1464, 371 - 37, 12690 - 2725, 132 - 12]\n",
        "}\n",
        "\n",
        "expected_data = {\n",
        "    'Register': ['Spoken', 'Spoken', 'Written', 'Written'],\n",
        "    'Passive Type': ['Be-passive', 'Get-passive', 'Be-passive', 'Get-passive'],\n",
        "    'With By-phrase': [2708.82, 28.18, 2708.82, 28.18],\n",
        "    'Without By-phrase': [9981.18, 103.82, 9981.18, 103.82]\n",
        "}\n",
        "\n",
        "# Convert observed and expected into DataFrames\n",
        "observed_df = pd.DataFrame(observed_data)\n",
        "expected_df = pd.DataFrame(expected_data)\n",
        "\n",
        "# Step 2: Calculate the standardized residuals\n",
        "observed_df['Std_Res_With_By'] = (observed_df['With By-phrase'] - expected_df['With By-phrase']) / np.sqrt(expected_df['With By-phrase'])\n",
        "observed_df['Std_Res_Without_By'] = (observed_df['Without By-phrase'] - expected_df['Without By-phrase']) / np.sqrt(expected_df['Without By-phrase'])\n",
        "\n",
        "# Step 3: Reshape the data for plotting (from wide to long format)\n",
        "df_melted = pd.melt(observed_df, id_vars=['Register', 'Passive Type'],\n",
        "                    value_vars=['Std_Res_With_By', 'Std_Res_Without_By'],\n",
        "                    var_name='By-phrase Status', value_name='Standardized Residual')\n",
        "\n",
        "# Step 4: Plot the diverging bar plot\n",
        "plt.figure(figsize=(12, 5))\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create a diverging bar plot using Seaborn without error bars\n",
        "palette = ['#FFD700', '#00008B']  # Custom palette for the bars\n",
        "sns.barplot(x='Standardized Residual', y='Register', hue='Passive Type', data=df_melted,\n",
        "            palette=palette, edgecolor=None, ci=None)\n",
        "\n",
        "# Customize the plot\n",
        "plt.axvline(0, color='black', linewidth=1)  # Add a vertical line at 0\n",
        "plt.title(\"Diverging Bar Plot of Standardized Residuals for By-phrase Usage\", fontsize=14)\n",
        "plt.xlabel(\"Standardized Residual\", fontsize=14)\n",
        "plt.xlim(-25,25)\n",
        "plt.ylabel(\"Register\", fontsize=14)\n",
        "plt.legend(title=\"Passive Type\", loc=\"upper right\")\n",
        "\n",
        "# Save the current figure in high resolution\n",
        "plt.savefig('/content/diverging_bar_byphrase.png', dpi=300, bbox_inches='tight')\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "NX17EJYnllod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis (0815)"
      ],
      "metadata": {
        "id": "xHafoAPn2ZeX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Be pp and Get pp columns"
      ],
      "metadata": {
        "id": "V7m0nAx42b0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/MK316/Getpp24/main/data/resultall0815-light.csv\"\n",
        "\n",
        "# Step 1: Read the dataframe from the provided URL\n",
        "df = pd.read_csv(url)"
      ],
      "metadata": {
        "id": "nBZqSLtd2gLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "4x5BPf8n2mRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas textblob matplotlib\n"
      ],
      "metadata": {
        "id": "JbRdq6eO3Pxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combine lists of sentences\n",
        "\n",
        "+ Bepplist.txt and Getpplist.text\n"
      ],
      "metadata": {
        "id": "PZb-5dUm3I6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from textblob import TextBlob\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load your dataframe\n",
        "# df = pd.read_csv('your_data.csv')  # Uncomment and modify if loading from a CSV\n",
        "\n",
        "# Combine lists into single strings for each column and save as text files\n",
        "with open('bepplist.txt', 'w') as f:\n",
        "    bepp_text = ' '.join([' '.join(item) for item in df['Bepp']])\n",
        "    f.write(bepp_text)\n",
        "\n",
        "with open('getpplist.txt', 'w') as f:\n",
        "    getpp_text = ' '.join([' '.join(item) for item in df['Getpp']])\n",
        "    f.write(getpp_text)\n",
        "\n",
        "# Perform sentiment analysis\n",
        "bepp_blob = TextBlob(bepp_text)\n",
        "getpp_blob = TextBlob(getpp_text)\n",
        "\n",
        "bepp_sentiment = bepp_blob.sentiment.polarity\n",
        "getpp_sentiment = getpp_blob.sentiment.polarity\n",
        "\n",
        "# Print sentiments\n",
        "print(f\"Sentiment polarity for Bepp: {bepp_sentiment}\")\n",
        "print(f\"Sentiment polarity for Getpp: {getpp_sentiment}\")\n",
        "\n",
        "# Plotting the sentiment results\n",
        "fig, ax = plt.subplots()\n",
        "ax.bar(['Bepp', 'Getpp'], [bepp_sentiment, getpp_sentiment], color=['blue', 'green'])\n",
        "ax.set_ylabel('Sentiment Polarity')\n",
        "ax.set_title('Sentiment Analysis of Bepp and Getpp Texts')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Ruv8qI-U3Ute"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from textblob import TextBlob\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming df is already loaded\n",
        "# df = pd.read_csv('your_data.csv')  # Uncomment and modify if loading from a CSV\n",
        "\n",
        "# Define function to combine texts based on category and register\n",
        "def combine_texts(df, category, register):\n",
        "    # Filter data based on register, matching exact case\n",
        "    filtered_texts = df[df['register'] == register.capitalize()]\n",
        "    print(f\"Filtered Texts for {category} ({register.capitalize()}):\", filtered_texts.shape)  # Check how many rows match the filter\n",
        "\n",
        "    # Combine all lists into one text\n",
        "    if not filtered_texts.empty:\n",
        "        combined_text = ' '.join([' '.join(item) for item in filtered_texts[category]])\n",
        "        return combined_text\n",
        "    return \"\"  # Return an empty string if no valid data is found\n",
        "\n",
        "# Combine texts for each category and register\n",
        "bepplist_written = combine_texts(df, 'Bepp', 'written')\n",
        "bepplist_spoken = combine_texts(df, 'Bepp', 'spoken')\n",
        "getpplist_written = combine_texts(df, 'Getpp', 'written')\n",
        "getpplist_spoken = combine_texts(df, 'Getpp', 'spoken')\n",
        "\n",
        "# Save combined texts to files\n",
        "texts_and_filenames = [\n",
        "    (bepplist_written, 'written-bepplist.txt'),\n",
        "    (bepplist_spoken, 'spoken-bepplist.txt'),\n",
        "    (getpplist_written, 'written-getpplist.txt'),\n",
        "    (getpplist_spoken, 'spoken-getpplist.txt')\n",
        "]\n",
        "\n",
        "for text, filename in texts_and_filenames:\n",
        "    with open(filename, 'w') as file:\n",
        "        file.write(text)\n",
        "\n",
        "# Perform sentiment analysis\n",
        "def analyze_sentiment(text):\n",
        "    blob = TextBlob(text)\n",
        "    return blob.sentiment.polarity\n",
        "\n",
        "# Collect sentiments\n",
        "categories = ['Written Bepp', 'Spoken Bepp', 'Written Getpp', 'Spoken Getpp']\n",
        "sentiments = {category: analyze_sentiment(text) for category, text in zip(categories, [bepplist_written, bepplist_spoken, getpplist_written, getpplist_spoken])}\n",
        "\n",
        "# Print sentiments\n",
        "for category, sentiment in sentiments.items():\n",
        "    print(f\"Sentiment polarity for {category}: {sentiment}\")\n",
        "\n",
        "# Plotting the sentiment results\n",
        "fig, ax = plt.subplots()\n",
        "ax.bar(categories, list(sentiments.values()), color=['blue', 'green', 'red', 'purple'])\n",
        "ax.set_ylabel('Sentiment Polarity')\n",
        "ax.set_title('Sentiment Analysis by Text and Register')\n",
        "plt.xticks(rotation=45)  # Rotate category labels for better readability\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-W0JELjA4FRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())  # Check the first few rows of the dataframe\n",
        "print(df['register'].unique())  # Check unique values in the 'register' column to ensure there's no case sensitivity or spacing issues\n",
        "\n"
      ],
      "metadata": {
        "id": "Z7R2zkai6Ti3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from textblob import TextBlob\n",
        "import ast\n",
        "\n",
        "# Function to safely convert string lists to actual lists and preprocess text\n",
        "def clean_and_convert_to_list(sentence_list_str):\n",
        "    try:\n",
        "        # Safe conversion from string to list\n",
        "        sentence_list = ast.literal_eval(sentence_list_str)\n",
        "        # Strip, remove excessive spaces and lower the case\n",
        "        cleaned_sentences = [sentence.strip().lower() for sentence in sentence_list if isinstance(sentence, str)]\n",
        "        return cleaned_sentences\n",
        "    except Exception as e:\n",
        "        print(\"Error processing:\", sentence_list_str, \"Error:\", e)\n",
        "        return []  # Return an empty list in case of an error\n",
        "\n",
        "# Apply cleaning function\n",
        "df['Bepp'] = df['Bepp'].apply(clean_and_convert_to_list)\n",
        "df['Getpp'] = df['Getpp'].apply(clean_and_convert_to_list)\n",
        "\n",
        "# Function to calculate average sentiment for lists of sentences\n",
        "def average_sentiment(sentence_list):\n",
        "    if not sentence_list:\n",
        "        return 0  # Handle empty lists after cleaning\n",
        "    sentiments = [TextBlob(sentence).sentiment.polarity for sentence in sentence_list]\n",
        "    return sum(sentiments) / len(sentiments) if sentiments else 0\n",
        "\n",
        "# Apply sentiment analysis\n",
        "df['Bepp_avg_sentiment'] = df['Bepp'].apply(average_sentiment)\n",
        "df['Getpp_avg_sentiment'] = df['Getpp'].apply(average_sentiment)\n",
        "\n",
        "# Print average sentiments\n",
        "print(\"Average 'Be-pp' Sentiment:\", df['Bepp_avg_sentiment'].mean())\n",
        "print(\"Average 'Get-pp' Sentiment:\", df['Getpp_avg_sentiment'].mean())\n"
      ],
      "metadata": {
        "id": "VomhR4CN6nHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "same code with number of sentences examined"
      ],
      "metadata": {
        "id": "RYEviKDlAIfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from textblob import TextBlob\n",
        "import ast\n",
        "\n",
        "# Function to safely convert string lists to actual lists and preprocess text\n",
        "def clean_and_convert_to_list(sentence_list_str):\n",
        "    try:\n",
        "        # Safe conversion from string to list\n",
        "        sentence_list = ast.literal_eval(sentence_list_str)\n",
        "        # Strip, remove excessive spaces and lower the case\n",
        "        cleaned_sentences = [sentence.strip().lower() for sentence in sentence_list if isinstance(sentence, str)]\n",
        "        return cleaned_sentences\n",
        "    except Exception as e:\n",
        "        print(\"Error processing:\", sentence_list_str, \"Error:\", e)\n",
        "        return []  # Return an empty list in case of an error\n",
        "\n",
        "# Apply cleaning function\n",
        "df['Bepp'] = df['Bepp'].apply(clean_and_convert_to_list)\n",
        "df['Getpp'] = df['Getpp'].apply(clean_and_convert_to_list)\n",
        "\n",
        "# Function to calculate average sentiment for lists of sentences and count them\n",
        "def average_sentiment_and_count(sentence_list):\n",
        "    count = len(sentence_list)\n",
        "    if not sentence_list:\n",
        "        return (0, 0)  # Handle empty lists after cleaning\n",
        "    sentiments = [TextBlob(sentence).sentiment.polarity for sentence in sentence_list]\n",
        "    average_sentiment = sum(sentiments) / len(sentiments) if sentiments else 0\n",
        "    return (average_sentiment, count)\n",
        "\n",
        "# Apply sentiment analysis and get counts\n",
        "df['Bepp_sentiment_and_count'] = df['Bepp'].apply(average_sentiment_and_count)\n",
        "df['Getpp_sentiment_and_count'] = df['Getpp'].apply(average_sentiment_and_count)\n",
        "\n",
        "# Extract average sentiments and counts\n",
        "df['Bepp_avg_sentiment'], df['Bepp_count'] = zip(*df['Bepp_sentiment_and_count'])\n",
        "df['Getpp_avg_sentiment'], df['Getpp_count'] = zip(*df['Getpp_sentiment_and_count'])\n",
        "\n",
        "# Print average sentiments and total sentence counts\n",
        "print(\"Average 'Be-pp' Sentiment:\", df['Bepp_avg_sentiment'].mean())\n",
        "print(\"Total 'Be-pp' sentences examined:\", df['Bepp_count'].sum())\n",
        "print(\"Average 'Get-pp' Sentiment:\", df['Getpp_avg_sentiment'].mean())\n",
        "print(\"Total 'Get-pp' sentences examined:\", df['Getpp_count'].sum())\n"
      ],
      "metadata": {
        "id": "5JGhWZAy_-5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sample 'Bepp' Data:\", df['Bepp'].head())\n",
        "print(\"Sample 'Getpp' Data:\", df['Getpp'].head())\n"
      ],
      "metadata": {
        "id": "khybtaj0-lj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Load English tokenizer, tagger, parser, NER and word vectors\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"She gets very excited about new projects.\"\n",
        "\n",
        "# Parse sentence using spaCy\n",
        "doc = nlp(sentence)\n",
        "\n",
        "# Analyze sentiment of the entire sentence\n",
        "overall_sentiment = TextBlob(sentence).sentiment.polarity\n",
        "print(f\"Overall Sentiment: {overall_sentiment}\")\n",
        "\n",
        "# Function to extract verb and dependent\n",
        "def extract_verb_and_dependent(doc):\n",
        "    for token in doc:\n",
        "        if \"VB\" in token.tag_:  # Checks for verbs\n",
        "            print(f\"Verb: {token.text}, Dependent: {[child.text for child in token.children]}\")\n",
        "\n",
        "# Run extraction\n",
        "extract_verb_and_dependent(doc)\n"
      ],
      "metadata": {
        "id": "olLwxxVU_bCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Assuming df['Bepp_avg_sentiment'] and df['Getpp_avg_sentiment'] are complete\n",
        "t_stat, p_val = ttest_ind(df['Bepp_avg_sentiment'].dropna(), df['Getpp_avg_sentiment'].dropna())\n",
        "\n",
        "print(f\"T-test statistic: {t_stat}, P-value: {p_val}\")\n"
      ],
      "metadata": {
        "id": "0lM6HMwP-24k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the first few elements to ensure they contain valid sentences\n",
        "print(df['Bepp'].head())\n",
        "print(df['Getpp'].head())\n"
      ],
      "metadata": {
        "id": "10ZRNwjZ-M-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Second trial (8/16 2am)"
      ],
      "metadata": {
        "id": "W2diDoV4A38Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/MK316/Getpp24/main/data/resultall0815-light.csv\"\n",
        "\n",
        "# Step 1: Read the dataframe from the provided URL\n",
        "df = pd.read_csv(url)"
      ],
      "metadata": {
        "id": "fS2FJcKKB0UU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "y22D9VbkC7hJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combine sentences by Bepp and Getpp (this works)"
      ],
      "metadata": {
        "id": "-u_NLERpEJYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample DataFrame initialization (Assuming df is already loaded with your actual data)\n",
        "# df = pd.read_csv('your_data.csv')  # If your data is in a CSV file\n",
        "\n",
        "def parse_and_combine_sentences(column):\n",
        "    all_sentences = []\n",
        "    for item in column:\n",
        "        try:\n",
        "            # Convert string list to actual list\n",
        "            sentences = ast.literal_eval(item)\n",
        "            all_sentences.extend(sentences)  # Combine all lists into one list of sentences\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing item: {item}, Error: {e}\")\n",
        "    return ' '.join(all_sentences)  # Return all sentences combined into a single string\n",
        "\n",
        "# Apply function to 'Bepp' and 'Getpp'\n",
        "bepp_text = parse_and_combine_sentences(df['Bepp'])\n",
        "getpp_text = parse_and_combine_sentences(df['Getpp'])\n",
        "\n",
        "# Save the combined texts to files\n",
        "with open('Belist.txt', 'w', encoding='utf-8') as f:\n",
        "    f.write(bepp_text)\n",
        "\n",
        "with open('Getlist.txt', 'w', encoding='utf-8') as f:\n",
        "    f.write(getpp_text)\n",
        "\n",
        "# Count sentences using NLTK's sent_tokenize\n",
        "bepp_sentences = len(sent_tokenize(bepp_text))\n",
        "getpp_sentences = len(sent_tokenize(getpp_text))\n",
        "\n",
        "# Print total number of sentences for each text\n",
        "print(\"Total number of sentences in 'Belist.txt':\", bepp_sentences)\n",
        "print(\"Total number of sentences in 'Getlist.txt':\", getpp_sentences)\n"
      ],
      "metadata": {
        "id": "AO6Dl_TECWWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textblob"
      ],
      "metadata": {
        "id": "0rRPI4IbEDU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "def perform_sentiment_analysis(filename):\n",
        "    # Read the text file\n",
        "    with open(filename, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    # Create a TextBlob object\n",
        "    blob = TextBlob(text)\n",
        "\n",
        "    # Calculate sentiment\n",
        "    sentiment = blob.sentiment\n",
        "\n",
        "    return sentiment\n",
        "\n",
        "# Perform sentiment analysis on each file\n",
        "bepp_sentiment = perform_sentiment_analysis('Belist.txt')\n",
        "getpp_sentiment = perform_sentiment_analysis('Getlist.txt')\n",
        "\n",
        "# Print the sentiment results\n",
        "print(\"Sentiment Analysis for 'Belist.txt':\")\n",
        "print(\"  Polarity: {:.4f}, Subjectivity: {:.4f}\".format(bepp_sentiment.polarity, bepp_sentiment.subjectivity))\n",
        "\n",
        "print(\"Sentiment Analysis for 'Getlist.txt':\")\n",
        "print(\"  Polarity: {:.4f}, Subjectivity: {:.4f}\".format(getpp_sentiment.polarity, getpp_sentiment.subjectivity))\n"
      ],
      "metadata": {
        "id": "NGr5krO8Ehv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "import nltk\n",
        "\n",
        "# Ensure necessary NLTK resources are downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "def perform_sentence_level_sentiment_analysis(filename):\n",
        "    # Read the text file\n",
        "    with open(filename, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    # Create a TextBlob object\n",
        "    blob = TextBlob(text)\n",
        "\n",
        "    # Analyze sentiment for each sentence\n",
        "    sentences = blob.sentences\n",
        "    sentiments = [(sentence, sentence.sentiment.polarity, sentence.sentiment.subjectivity) for sentence in sentences]\n",
        "\n",
        "    return sentiments\n",
        "\n",
        "# Perform sentence-level sentiment analysis on each file\n",
        "bepp_sentiments = perform_sentence_level_sentiment_analysis('Belist.txt')\n",
        "getpp_sentiments = perform_sentence_level_sentiment_analysis('Getlist.txt')\n",
        "\n",
        "# Print the sentiment results for each sentence\n",
        "print(\"Sentence-level Sentiment Analysis for 'Belist.txt':\")\n",
        "for sentence, polarity, subjectivity in bepp_sentiments:\n",
        "    print(f\"Sentence: {sentence}\\nPolarity: {polarity:.4f}, Subjectivity: {subjectivity:.4f}\\n\")\n",
        "\n",
        "print(\"Sentence-level Sentiment Analysis for 'Getlist.txt':\")\n",
        "for sentence, polarity, subjectivity in getpp_sentiments:\n",
        "    print(f\"Sentence: {sentence}\\nPolarity: {polarity:.4f}, Subjectivity: {subjectivity:.4f}\\n\")\n"
      ],
      "metadata": {
        "id": "8x9AWgPLE_Ia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from textblob import TextBlob\n",
        "import nltk\n",
        "\n",
        "# Ensure necessary NLTK resources are downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "def perform_sentence_level_sentiment_analysis(filename):\n",
        "    # Read the text file\n",
        "    with open(filename, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    # Create a TextBlob object\n",
        "    blob = TextBlob(text)\n",
        "\n",
        "    # Analyze sentiment for each sentence\n",
        "    sentences = blob.sentences\n",
        "    sentiments = [{'Sentence': str(sentence), 'Polarity': sentence.sentiment.polarity, 'Subjectivity': sentence.sentiment.subjectivity, 'Source': filename} for sentence in sentences]\n",
        "\n",
        "    return sentiments\n",
        "\n",
        "# Perform sentence-level sentiment analysis on each file\n",
        "bepp_sentiments = perform_sentence_level_sentiment_analysis('Belist.txt')\n",
        "getpp_sentiments = perform_sentence_level_sentiment_analysis('Getlist.txt')\n",
        "\n",
        "# Combine the results into a single DataFrame\n",
        "sentiment_data = pd.DataFrame(bepp_sentiments + getpp_sentiments)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "sentiment_data.to_csv('sentence_level_sentiments.csv', index=False)\n",
        "\n",
        "print(\"Sentiment analysis results saved to 'sentence_level_sentiments.csv'.\")\n"
      ],
      "metadata": {
        "id": "MiuydRFGFJuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_data.head()"
      ],
      "metadata": {
        "id": "gx7DtPnxG4Db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_data.tail()"
      ],
      "metadata": {
        "id": "XeFBY6VKGtqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib pandas\n"
      ],
      "metadata": {
        "id": "Qrfy7pmmIfff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'sentiment_data' is already loaded into your Colab session\n",
        "# If you need to load it from a CSV, you can uncomment the next line\n",
        "# sentiment_data = pd.read_csv('path_to_your_csv.csv')\n",
        "\n",
        "# Function to categorize polarity into 'negative', 'neutral', 'positive'\n",
        "def categorize_polarity(polarity):\n",
        "    if polarity < 0:\n",
        "        return 'Negative'\n",
        "    elif polarity == 0:\n",
        "        return 'Neutral'\n",
        "    else:\n",
        "        return 'Positive'\n",
        "\n",
        "# Apply the categorization function to the 'Polarity' column\n",
        "sentiment_data['Polarity Category'] = sentiment_data['Polarity'].apply(categorize_polarity)\n",
        "\n",
        "# Group data by 'Source' and 'Polarity Category' and count occurrences\n",
        "polarity_counts = sentiment_data.groupby(['Source', 'Polarity Category']).size().unstack(fill_value=0)\n",
        "\n",
        "# Plotting\n",
        "fig, axes = plt.subplots(nrows=1, ncols=len(polarity_counts.index), figsize=(12, 5), sharey=True)\n",
        "\n",
        "for ax, (source, counts) in zip(axes, polarity_counts.iterrows()):\n",
        "    counts.plot(kind='bar', ax=ax, title=f'Polarity Counts for {source}')\n",
        "    ax.set_xlabel('Polarity Category')\n",
        "    ax.set_ylabel('Count')\n",
        "    ax.set_ylim(0, max(polarity_counts.max()) + 10)  # Adjust y-limits for better visualization\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kdAh9i_MInJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combine sentences by register and passive types\n",
        "\n",
        "Keeping the above steps"
      ],
      "metadata": {
        "id": "IhGAc82gJhXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample DataFrame initialization (Assuming df is already loaded with your actual data)\n",
        "# df = pd.read_csv('your_data.csv')  # If your data is in a CSV file\n",
        "\n",
        "def parse_and_combine_sentences(data):\n",
        "    all_sentences = []\n",
        "    for item in data:\n",
        "        try:\n",
        "            # Convert string list to actual list\n",
        "            sentences = ast.literal_eval(item)\n",
        "            all_sentences.extend(sentences)  # Combine all lists into one list of sentences\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing item: {item}, Error: {e}\")\n",
        "    return ' '.join(all_sentences)  # Return all sentences combined into a single string\n",
        "\n",
        "# Function to process texts by register and category\n",
        "def process_texts_by_register_and_category(df, category, register):\n",
        "    # Filter data by register\n",
        "    filtered_data = df[df['register'] == register][category]\n",
        "    combined_text = parse_and_combine_sentences(filtered_data)\n",
        "    # Save the combined texts to files\n",
        "    filename = f'{category}_{register}.txt'\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        f.write(combined_text)\n",
        "    # Count sentences using NLTK's sent_tokenize\n",
        "    sentence_count = len(sent_tokenize(combined_text))\n",
        "    print(f\"Total number of sentences in '{filename}':\", sentence_count)\n",
        "\n",
        "# Apply function to 'Bepp' and 'Getpp' for each register\n",
        "registers = ['Spoken', 'Written']\n",
        "categories = ['Bepp', 'Getpp']\n",
        "for category in categories:\n",
        "    for register in registers:\n",
        "        process_texts_by_register_and_category(df, category, register)\n"
      ],
      "metadata": {
        "id": "qwi0IRcnJo-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Record the sentiment result in dataframe (passive types, register)"
      ],
      "metadata": {
        "id": "9uIbYRLsKIkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from textblob import TextBlob\n",
        "import nltk\n",
        "import os\n",
        "\n",
        "# Ensure necessary NLTK resources are downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "def perform_sentence_level_sentiment_analysis(filename):\n",
        "    # Read the text file\n",
        "    with open(filename, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    # Extract the register from the filename assuming format 'Category_Register.txt'\n",
        "    register = filename.split('_')[-1].split('.')[0]  # This splits on underscore and dot, and gets the register part\n",
        "\n",
        "    # Create a TextBlob object\n",
        "    blob = TextBlob(text)\n",
        "\n",
        "    # Analyze sentiment for each sentence\n",
        "    sentences = blob.sentences\n",
        "    sentiments = [{\n",
        "        'Sentence': str(sentence),\n",
        "        'Polarity': sentence.sentiment.polarity,\n",
        "        'Subjectivity': sentence.sentiment.subjectivity,\n",
        "        'Source': os.path.basename(filename).split('_')[0],  # Extracts 'Bepp' or 'Getpp' from filename\n",
        "        'Register': register.capitalize()  # Capitalize the register for consistency\n",
        "    } for sentence in sentences]\n",
        "\n",
        "    return sentiments\n",
        "\n",
        "# Perform sentence-level sentiment analysis on each file\n",
        "files = ['Bepp_Spoken.txt', 'Bepp_Written.txt', 'Getpp_Spoken.txt', 'Getpp_Written.txt']\n",
        "all_sentiments = []\n",
        "for file in files:\n",
        "    sentiments = perform_sentence_level_sentiment_analysis(file)\n",
        "    all_sentiments.extend(sentiments)\n",
        "\n",
        "# Combine the results into a single DataFrame\n",
        "sentiment_data = pd.DataFrame(all_sentiments)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "sentiment_data.to_csv('Total_sentence_level_sentiments.csv', index=False)\n",
        "\n",
        "print(\"Sentiment analysis results saved to 'sentence_level_sentiments.csv'.\")\n"
      ],
      "metadata": {
        "id": "Huf2MlfsKQvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[result data](https://github.com/MK316/Getpp24/blob/main/data/Total_sentence_level_sentiments.csv) too big to read?"
      ],
      "metadata": {
        "id": "m7XKejvgLtSr"
      }
    }
  ]
}